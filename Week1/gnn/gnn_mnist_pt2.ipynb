{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    " \n",
    "from tensorboardX import SummaryWriter #interface between PyTorch and TensorBoard\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(nn.Module): #a custom stack that defines a flexible Graph Neural Network (GNN) architecture\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, task='node'):\n",
    "        super(GNNStack, self).__init__() #initializes the GNNStack class, which inherits from nn.Module\n",
    "        self.task = task #task can be 'node' for node classification or 'graph' for graph classification\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim)) #first GNN convolution layer\n",
    "        self.lns = nn.ModuleList() #a list to hold layer normalization layers for stabilization\n",
    "        self.lns.append(nn.LayerNorm(hidden_dim)) #used after first conv layer\n",
    "        self.lns.append(nn.LayerNorm(hidden_dim)) #used after second conv layer\n",
    "        for l in range(2):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim)) #adds two more GNN convolution layers to the stack\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential( \n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), \n",
    "            nn.Linear(hidden_dim, output_dim)) \n",
    "        #a feedfoward multilayer perceptron (MLP) that takes the output of the last GNN layer and applies two linear transformations with a dropout in between.\n",
    "        if not (self.task == 'node' or self.task == 'graph'):\n",
    "            raise RuntimeError('Unknown task.')\n",
    "\n",
    "        self.dropout = 0.25\n",
    "        self.num_layers = 3 \n",
    "\n",
    "    def build_conv_model(self, input_dim, hidden_dim): #returns a convolutional layer based on the task type\n",
    "        # refer to pytorch geometric nn module for different implementation of GNNs.\n",
    "        if self.task == 'node':\n",
    "            return pyg_nn.GCNConv(input_dim, hidden_dim) #used for node classification tasks\n",
    "        else:\n",
    "            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))) #used for graph classification tasks\n",
    "\n",
    "    def forward(self, data): #excpects a torch_geometric.data.Data object as input, which contains the graph structure and node features\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch #x is node features, edge_index is the connectivity of the graph, and batch is batch vector indicating which node belongs to which graph\n",
    "        if data.num_node_features == 0:\n",
    "          x = torch.ones(data.num_nodes, 1) #if no node features, initilize nodes with ones\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)  #three layers of convolution, edge_index is the connectivity of the graph\n",
    "            emb = x\n",
    "            x = F.relu(x) \n",
    "            x = F.dropout(x, p=self.dropout, training=self.training) #makes the model robust to overfitting by randomly setting a fraction of input units to 0 during training\n",
    "            if not i == self.num_layers - 1: #means we are not at the last layer\n",
    "                x = self.lns[i](x) #layer normalization to stabilize the learning process by normalizing the inputs to a layer for each mini-batch\n",
    "\n",
    "        if self.task == 'graph':\n",
    "            x = pyg_nn.global_mean_pool(x, batch) #global pooling operation to aggregate node features into a single graph-level representation\n",
    "\n",
    "        x = self.post_mp(x) # self.post_mp is a sequential model that applies two linear transformations with a dropout in between so that the output of the last layer is transformed into the desired output dimension.\n",
    "\n",
    "        return emb, F.log_softmax(x, dim=1) #emb is the intermediate node embeddings, and the second part is the output of the model after applying a log softmax function to the final layer's output, which is useful for multi-class classification tasks.\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label) #calculates the negative log likelihood loss between the predicted and true labels, which is commonly used for classification tasks in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CustomConv, self).__init__(aggr='add') # \"Add\" aggregation.\n",
    "        self.lin = nn.Linear(in_channels, out_channels) # Linear transformation for node features.\n",
    "        self.lin_self = nn.Linear (in_channels, out_channels) # Linear transformation for self-loops.\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels], this means that x is a matrix where each row corresponds to a node and each column corresponds to a feature.\n",
    "        # edge_index has shape [2, E], where E is the number of edges.\n",
    "\n",
    "        #add self loops to the adjacency matrix\n",
    "        edge_index, _ = pyg_utils.remove_self_loops(edge_index) #removes self-loops from the edge index, the _ is a placeholder for the removed self-loops\n",
    "        \n",
    "        #transform node feature matrix\n",
    "        self_x = self.lin_self(x) #applies the linear transformation to the node features\n",
    "        x = self.lin(x) #applies the linear transformation to the node features\n",
    "\n",
    "        return self_x + self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x) #propagates messages through the graph and aggregates them using the 'add' aggregation method, then adds the self-loop features to the aggregated features.\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index, size):\n",
    "        # x_i has shape [E, in_channels], where E is the number of edges.\n",
    "        # x_j has shape [E, in_channels], where E is the number of edges.\n",
    "        # edge_index has shape [2, E], where E is the number of edges.\n",
    "\n",
    "        row, col = edge_index\n",
    "        # row contains the source nodes and col contains the target nodes of the edges.\n",
    "        deg = pyg_utils.degree(row, size=size[0], dtype=x_j.dtype) #this calculates the degree of each node in the graph, which is the number of edges connected to each node. The size parameter specifies the number of nodes in the graph, and dtype ensures that the degree tensor has the same data type as x_j.\n",
    "        deg_inv_sqrt = deg.pow(-0.5) #calculates the inverse square root of the degree of each node, which is used to normalize the messages sent along the edges\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col] #calculates the normalization factor for each edge based on the degrees of the source and target nodes, which is used to normalize the messages sent along the edges.  \n",
    "        return x_j\n",
    "    \n",
    "    def update(self, aggr_out):\n",
    "        #aggr_out has shape [N, out_channels], where N is the number of nodes and out_channels is the number of output channels.\n",
    "        return aggr_out #returns the aggregated output, which is the result of the message passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8369469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv1)",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
